# TranslateGemma Docker Configuration

# Service port (default: 8022)
PORT=8022

# GPU selection (auto-selected by start.sh, or set manually)
# Use nvidia-smi to check available GPUs
NVIDIA_VISIBLE_DEVICES=0

# Model configuration
# Model size: 4b, 12b, 27b (default: 12b - best balance)
MODEL_NAME=12b

# Quantization: 4 or 8 (default: 4 - faster, less VRAM)
QUANTIZATION=4

# Backend: gguf, pytorch (default: gguf - fastest)
BACKEND=gguf

# GPU idle timeout in seconds (default: 300 = 5 minutes)
# Model will be unloaded after this idle time to free VRAM
GPU_IDLE_TIMEOUT=300

# Max chunk length for text splitting (default: 80)
MAX_CHUNK_LENGTH=80

# Model cache path (default: ~/.cache/translate/models)
# Models will be downloaded here on first run
MODEL_PATH=~/.cache/translate/models

# HuggingFace settings (optional)
# HF_ENDPOINT=https://huggingface.co
# HF_TOKEN=your_token_here
