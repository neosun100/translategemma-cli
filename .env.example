# TranslateGemma Configuration
# Copy this file to .env and modify as needed

# Service port (default: 8022)
PORT=8022

# GPU selection (use nvidia-smi to check available GPUs)
NVIDIA_VISIBLE_DEVICES=0

# Model configuration
# Model size: 4b, 12b, 27b (default: 27b - best quality)
MODEL_NAME=27b

# Quantization: 4 or 8 (default: 8 - higher quality)
QUANTIZATION=8

# Backend: gguf, pytorch (default: gguf - fastest)
BACKEND=gguf

# GPU idle timeout in seconds (default: 0 = immediate unload)
# Model will be unloaded after this idle time to free VRAM
GPU_IDLE_TIMEOUT=0

# Max chunk length for text splitting (default: 100 - safe boundary)
# WARNING: Values > 100 may cause translation truncation
MAX_CHUNK_LENGTH=100

# Sliding window overlap (default: 0 = disabled)
# TranslateGemma maintains context automatically, overlap not needed
DEFAULT_OVERLAP=0

# Repetition penalty (default: 1.0 = no penalty)
REPETITION_PENALTY=1.0

# HuggingFace settings (optional, for model download)
# HF_ENDPOINT=https://huggingface.co
# HF_TOKEN=your_token_here
