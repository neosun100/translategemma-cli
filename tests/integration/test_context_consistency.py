#!/usr/bin/env python3
"""
æµ‹è¯•ï¼šæ— æ»‘åŠ¨çª—å£æ—¶çš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§é—®é¢˜
éªŒè¯è·¨ chunk è¾¹ç•Œæ—¶çš„ç¿»è¯‘ä¸€è‡´æ€§
"""
import requests

API_URL = "http://localhost:8022/api/translate"

# æµ‹è¯•ç”¨ä¾‹ï¼šåŒ…å«éœ€è¦ä¸Šä¸‹æ–‡æ‰èƒ½æ­£ç¡®ç¿»è¯‘çš„å†…å®¹
TEST_CASES = [
    {
        "name": "ä»£è¯æŒ‡ä»£ä¸€è‡´æ€§",
        "text": """å¼ ä¸‰æ˜¯ä¸€ä½è‘—åçš„äººå·¥æ™ºèƒ½ç ”ç©¶å‘˜ã€‚ä»–åœ¨æ¸…åå¤§å­¦è·å¾—åšå£«å­¦ä½ã€‚ä»–çš„ç ”ç©¶æ–¹å‘æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ã€‚ä»–å‘è¡¨äº†è¶…è¿‡100ç¯‡è®ºæ–‡ã€‚ä»–çš„å­¦ç”Ÿéå¸ƒä¸–ç•Œå„åœ°ã€‚""",
        "check": "ä»£è¯'ä»–'æ˜¯å¦å§‹ç»ˆç¿»è¯‘ä¸ºåŒä¸€ä¸ªè¯(He/His)",
    },
    {
        "name": "ä¸“æœ‰åè¯ä¸€è‡´æ€§",
        "text": """æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ã€‚æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œã€‚æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«é¢†åŸŸå–å¾—äº†çªç ´ã€‚æ·±åº¦å­¦ä¹ ä¹Ÿè¢«åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ã€‚æ·±åº¦å­¦ä¹ çš„å‘å±•æ¨åŠ¨äº†äººå·¥æ™ºèƒ½çš„è¿›æ­¥ã€‚""",
        "check": "'æ·±åº¦å­¦ä¹ 'æ˜¯å¦å§‹ç»ˆç¿»è¯‘ä¸º'deep learning'",
    },
    {
        "name": "æœ¯è¯­ä¸€è‡´æ€§ (è·¨æ®µè½)",
        "text": """å·ç§¯ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§ç‰¹æ®Šçš„ç¥ç»ç½‘ç»œç»“æ„ã€‚å®ƒæœ€åˆè¢«è®¾è®¡ç”¨äºå›¾åƒå¤„ç†ä»»åŠ¡ã€‚å·ç§¯ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒæ˜¯å·ç§¯å±‚å’Œæ± åŒ–å±‚ã€‚é€šè¿‡è¿™äº›å±‚ï¼Œå·ç§¯ç¥ç»ç½‘ç»œå¯ä»¥è‡ªåŠ¨æå–å›¾åƒç‰¹å¾ã€‚å¦‚ä»Šï¼Œå·ç§¯ç¥ç»ç½‘ç»œå·²ç»æˆä¸ºè®¡ç®—æœºè§†è§‰çš„åŸºç¡€æ¶æ„ã€‚""",
        "check": "'å·ç§¯ç¥ç»ç½‘ç»œ'æ˜¯å¦å§‹ç»ˆç¿»è¯‘ä¸€è‡´(CNN/Convolutional Neural Network)",
    },
    {
        "name": "ä¸Šä¸‹æ–‡ä¾èµ–ç¿»è¯‘",
        "text": """è‹¹æœå…¬å¸å‘å¸ƒäº†æ–°äº§å“ã€‚è¿™æ¬¾äº§å“é‡‡ç”¨äº†æœ€æ–°çš„èŠ¯ç‰‡æŠ€æœ¯ã€‚è‹¹æœè¡¨ç¤ºè¿™æ˜¯ä»–ä»¬æœ€å¼ºå¤§çš„è®¾å¤‡ã€‚åˆ†æå¸ˆè®¤ä¸ºè‹¹æœçš„å¸‚åœºä»½é¢å°†ç»§ç»­å¢é•¿ã€‚è‹¹æœçš„è‚¡ä»·åœ¨å‘å¸ƒä¼šåä¸Šæ¶¨äº†5%ã€‚""",
        "check": "'è‹¹æœ'æ˜¯å¦å§‹ç»ˆç¿»è¯‘ä¸º'Apple'(å…¬å¸)è€Œé'apple'(æ°´æœ)",
    },
    {
        "name": "æ€§åˆ«ä»£è¯ä¸€è‡´æ€§",
        "text": """ç›ä¸½æ˜¯ä¸€ä½æ°å‡ºçš„ç§‘å­¦å®¶ã€‚å¥¹åœ¨MITå®Œæˆäº†åšå£«åç ”ç©¶ã€‚å¥¹çš„å¯¼å¸ˆæ˜¯è¯ºè´å°”å¥–å¾—ä¸»ã€‚å¥¹ç›®å‰åœ¨æ–¯å¦ç¦å¤§å­¦ä»»æ•™ã€‚å¥¹çš„ç ”ç©¶æ”¹å˜äº†æˆ‘ä»¬å¯¹å®‡å®™çš„ç†è§£ã€‚""",
        "check": "ä»£è¯'å¥¹'æ˜¯å¦å§‹ç»ˆç¿»è¯‘ä¸º'She/Her'",
    },
]

def test_consistency(text: str, overlap: int) -> str:
    """ç¿»è¯‘å¹¶è¿”å›ç»“æœ"""
    payload = {
        "text": text,
        "target_lang": "en",
        "chunk_size": 100,
        "overlap": overlap,
    }
    resp = requests.post(API_URL, json=payload)
    data = resp.json()
    return data.get("result", ""), data.get("chunks", 0)

def analyze_consistency(result: str, term_pairs: list) -> dict:
    """åˆ†æç¿»è¯‘ä¸€è‡´æ€§"""
    analysis = {}
    for cn, en_options in term_pairs:
        found = []
        for en in en_options:
            count = result.lower().count(en.lower())
            if count > 0:
                found.append((en, count))
        analysis[cn] = found
    return analysis

def main():
    print("=" * 70)
    print("ä¸Šä¸‹æ–‡ä¸€è‡´æ€§æµ‹è¯•ï¼šæœ‰æ— æ»‘åŠ¨çª—å£å¯¹æ¯”")
    print("=" * 70)
    
    for case in TEST_CASES:
        print(f"\n{'='*70}")
        print(f"æµ‹è¯•: {case['name']}")
        print(f"æ£€æŸ¥ç‚¹: {case['check']}")
        print(f"åŸæ–‡: {case['text'][:80]}...")
        print("-" * 70)
        
        # æ— æ»‘åŠ¨çª—å£
        result_no, chunks_no = test_consistency(case["text"], 0)
        print(f"\nã€æ— æ»‘åŠ¨çª—å£ã€‘(overlap=0, {chunks_no} chunks)")
        print(f"ç¿»è¯‘: {result_no}")
        
        # æœ‰æ»‘åŠ¨çª—å£
        result_with, chunks_with = test_consistency(case["text"], 20)
        print(f"\nã€æœ‰æ»‘åŠ¨çª—å£ã€‘(overlap=20, {chunks_with} chunks)")
        print(f"ç¿»è¯‘: {result_with}")
        
        # ç®€å•å¯¹æ¯”
        print(f"\nğŸ“Š å¯¹æ¯”:")
        print(f"  æ— overlapé•¿åº¦: {len(result_no)}")
        print(f"  æœ‰overlapé•¿åº¦: {len(result_with)} ({len(result_with)-len(result_no):+d})")
    
    print("\n" + "=" * 70)
    print("æµ‹è¯•å®Œæˆ - è¯·äººå·¥æ£€æŸ¥ä¸Šè¿°ç¿»è¯‘çš„ä¸€è‡´æ€§")
    print("=" * 70)

if __name__ == "__main__":
    main()
